from __future__ import annotations

import json
import os
from pathlib import Path
from typing import Any, Dict, Optional, Type

from crewai.tools import BaseTool
from pydantic import BaseModel, Field


def _read_repo_text(relative_path: str) -> str:
    """Best-effort read from repo root; returns empty string on failure."""
    try:
        repo_root = Path(__file__).resolve().parents[3]
        p = repo_root / relative_path
        return p.read_text(encoding="utf-8").strip()
    except Exception:
        return ""


# Mandatory disclaimer required by the project spec.
_FALLBACK_DISCLAIMER = (
    "This plan is generated by an AI language model and is not a medical diagnosis or clinician recommendation."
)


class LLMFallbackPlanInput(BaseModel):
    """Input schema for the LLM fallback plan tool."""

    dialogue: str = Field(..., description="Full doctor-patient dialogue transcript")
    assessment_plan_json: str = Field(
        ..., description="Strict JSON output from assessment_plan_task as a string"
    )


class LLMFallbackPlanTool(BaseTool):
    name: str = "llm_fallback_plan"
    description: str = (
        "Generates a conservative, non-diagnostic care plan suggestion ONLY when no dataset plan is available. "
        "Must include disclaimer and avoid antibiotics, diagnoses, and dosing beyond label instructions."
    )
    args_schema: Type[BaseModel] = LLMFallbackPlanInput

    def _is_high_risk(self, dialogue: str, problems: list[str]) -> bool:
        text = " ".join(str(dialogue or "").lower().split())
        probs = {" ".join(str(p).lower().split()) for p in (problems or []) if str(p).strip()}

        # Explicit high-risk contexts.
        high_risk_markers = {
            "cancer",
            "chemotherapy",
            "chemo",
            "immunocompromised",
            "immunosuppressed",
            "transplant",
            "seizure",
            "stroke",
            "paralysis",
            "weakness",
            "numbness",
            "confusion",
            "fainted",
            "unconscious",
            "vomiting blood",
            "blood in vomit",
            "black stool",
            "blood in stool",
            "chest pain",
            "short of breath",
            "shortness of breath",
        }
        if any(m in text for m in high_risk_markers):
            return True
        if any(any(m in p for m in high_risk_markers) for p in probs):
            return True
        return False

    def _safe_template_plan(self, dialogue: str, problems: list[str]) -> str:
        # Conservative, non-diagnostic, label-only OTC advice. No dosing.
        text = " ".join(str(dialogue or "").lower().split())
        probs = {" ".join(str(p).lower().split()) for p in (problems or []) if str(p).strip()}

        mentions_paracetamol = "paracetamol" in text or "acetaminophen" in text

        lines: list[str] = []
        lines.append("Rest and stay well-hydrated.")
        lines.append("Monitor your symptoms and seek clinician review if they persist or worsen.")

        if mentions_paracetamol:
            lines.append("You can continue paracetamol/acetaminophen only as per the package label instructions.")
        else:
            lines.append("If needed, you may use over-the-counter pain/fever relief only as per the package label instructions.")

        # Add non-presumptive return precautions based on common low-risk symptom clusters.
        if "fever" in probs or "headache" in probs or "temperature" in probs:
            lines.append(
                "Seek urgent care if you develop severe headache, neck stiffness, confusion, chest pain, shortness of breath, a widespread rash, or if symptoms rapidly worsen."
            )
        else:
            lines.append("Seek urgent care if you develop severe or rapidly worsening symptoms.")

        return "\n".join(lines).strip()

    def _run(self, dialogue: str, assessment_plan_json: str) -> Dict[str, Any]:
        # Return only the mandatory disclaimer string for llm_fallback outputs.
        disclaimer = _FALLBACK_DISCLAIMER

        model = (
            os.getenv("LITELLM_MODEL")
            or os.getenv("OPENAI_MODEL")
            or os.getenv("OPENAI_MODEL_NAME")
            or os.getenv("MODEL")
            or "gpt-4o-mini"
        )

        # Parse problems (extractive) for risk gating and template fallback.
        problems: list[str] = []
        try:
            parsed = json.loads(assessment_plan_json)
            if isinstance(parsed, dict):
                assessment = parsed.get("assessment")
                if isinstance(assessment, list):
                    for item in assessment:
                        if isinstance(item, dict) and item.get("problem"):
                            problems.append(str(item.get("problem")))
        except Exception:
            problems = []

        # High-risk: do not generate a detailed plan (even via LLM).
        if self._is_high_risk(dialogue, problems):
            return {
                "plan_text": "Insufficient information to safely suggest a care plan.",
                "disclaimer": disclaimer,
                "safety_notes": "High-risk context detected; no diagnosis and no prescriptions.",
            }

        system = (
            "You are generating a conservative care plan suggestion for a clinical note-taking tool. "
            "You MUST follow these safety rules:\n"
            "- Do NOT diagnose or speculate a cause.\n"
            "- Do NOT recommend antibiotics or prescription-only meds.\n"
            "- Do NOT provide dosing beyond package label instructions.\n"
            "- Use only general self-care, monitoring, return-precautions, and advice to consult a clinician.\n"
            "- If information is insufficient, say so and give only high-level safe next steps.\n"
            "Return STRICT JSON only with keys: plan_text, disclaimer, safety_notes.\n"
            "plan_text should be short bullet-style lines separated by newline characters."
        )

        user = (
            "INPUT DIALOGUE:\n"
            f"{dialogue}\n\n"
            "STRICT ASSESSMENT/PLAN JSON (may contain plan='insufficient information'):\n"
            f"{assessment_plan_json}\n\n"
            "Generate a conservative care plan suggestion that does not exceed the input information."
        )

        try:
            from litellm import completion  # type: ignore

            resp = completion(
                model=model,
                messages=[
                    {"role": "system", "content": system},
                    {"role": "user", "content": user},
                ],
                temperature=0.2,
                response_format={"type": "json_object"},
            )

            content = (
                resp.get("choices", [{}])[0]
                .get("message", {})
                .get("content", "")
                .strip()
            )
            data: Dict[str, Any] = json.loads(content)
        except Exception as e:
            # If the model is not configured, still return a transparent, safe output
            # but do not over-block: provide a conservative template plan.
            data = {
                "plan_text": self._safe_template_plan(dialogue, problems),
                "disclaimer": disclaimer,
                "safety_notes": f"LLM fallback unavailable: {type(e).__name__}; returned conservative template plan.",
            }

        # Enforce disclaimer regardless of what the model returned.
        data.setdefault("plan_text", "")
        data["disclaimer"] = disclaimer
        data.setdefault(
            "safety_notes",
            "No diagnosis. No antibiotics. No dosing beyond label instructions.",
        )

        return data
